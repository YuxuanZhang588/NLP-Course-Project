{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "#### Yuxuan Zhang\n",
    "\n",
    "#### We will first run the script to display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bass', 'bass', 'bass', 'tuba', 'tuba', 'tuba', 'deep', 'bass', 'horn', 'horn', 'tuba', 'horn', 'tuba', 'deep', 'deep', 'horn', 'deep', 'horn', 'deep', 'bass', 'horn', 'deep', 'horn', 'deep', 'deep', 'tuba', 'horn', 'horn', 'tuba', 'bass', 'horn', 'horn', 'horn', 'horn', 'deep', 'horn', 'deep', 'tuba', 'bass', 'horn', 'deep', 'horn', 'bass', 'deep', 'tuba', 'tuba', 'horn', 'tuba', 'bass', 'horn', 'horn', 'horn', 'tuba', 'bass']\n",
      "[0.33333334 0.33333334 0.33333334]\n",
      "[(0, '0.361*\"pike\" + 0.272*\"bass\" + 0.197*\"deep\" + 0.071*\"horn\" + 0.064*\"catapult\" + 0.035*\"tuba\"'), (1, '0.255*\"bass\" + 0.221*\"horn\" + 0.183*\"deep\" + 0.129*\"tuba\" + 0.127*\"pike\" + 0.085*\"catapult\"'), (2, '0.376*\"pike\" + 0.263*\"catapult\" + 0.209*\"horn\" + 0.077*\"deep\" + 0.059*\"bass\" + 0.016*\"tuba\"')]\n"
     ]
    }
   ],
   "source": [
    "!python ./lda_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For inferred topic 0, we have\n",
    "- bass - 0.272\n",
    "- pike - 0.361\n",
    "- deep - 0.197\n",
    "- tuba - 0.035\n",
    "- horn - 0.071\n",
    "- catapult - 0.064\n",
    "\n",
    "#### We see that this topic has dominant words \"pike\", \"bass\", and \"deep\", and the two words with low probability are \"horn\" and \"tuba\". Hence, this should be mapped to the true topic 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For inferred topic 1, we have\n",
    "- bass - 0.255\n",
    "- pike - 0.127\n",
    "- deep - 0.183\n",
    "- tuba - 0.129\n",
    "- horn - 0.221\n",
    "- catapult - 0.085\n",
    "\n",
    "#### We see that this topic has dominant words \"bass\", \"horn\", \"deep\", and \"tuba\", so this topic should likely be mapped to the true topic 2. However, we also see a bit of noise coming from \"pike\", which appears in this inferred topic with a lower probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For inferred topic 2, we have\n",
    "- bass - 0.059\n",
    "- pike - 0.376\n",
    "- deep - 0.077\n",
    "- tuba - 0.016\n",
    "- horn - 0.209\n",
    "- catapult - 0.263\n",
    "\n",
    "#### We see that this topic has dominant words \"pike\", \"catapult\", and \"horn\". The word \"deep\" also has a probability of 0.077, which is close to 0.1. Therefore, this topic should be mapped to the true topic 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In conclusion, the mapping between our inferred result and true topics is\n",
    "\n",
    "Inferred topic 0 &rarr; True topic 0\n",
    "\n",
    "Inferred topic 1 &rarr; True topic 2\n",
    "\n",
    "Inferred topic 2 &rarr; True topic 1\n",
    "\n",
    "\n",
    "#### To reduce the noise, we can increase the number of iterations of the 'ldaModel'. From the documentation of gensim, we see that this parameter is set to a default value of 50, which may be the reason for the noise in our result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
