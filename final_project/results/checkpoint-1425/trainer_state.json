{
  "best_metric": 1.3078809976577759,
  "best_model_checkpoint": "./results\\checkpoint-1425",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1425,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021052631578947368,
      "grad_norm": 2.9554367065429688,
      "learning_rate": 4.9649122807017544e-05,
      "loss": 2.0953,
      "step": 10
    },
    {
      "epoch": 0.042105263157894736,
      "grad_norm": 3.3791446685791016,
      "learning_rate": 4.9298245614035086e-05,
      "loss": 1.9307,
      "step": 20
    },
    {
      "epoch": 0.06315789473684211,
      "grad_norm": 4.273809909820557,
      "learning_rate": 4.8947368421052635e-05,
      "loss": 1.8044,
      "step": 30
    },
    {
      "epoch": 0.08421052631578947,
      "grad_norm": 5.097339630126953,
      "learning_rate": 4.859649122807018e-05,
      "loss": 2.0665,
      "step": 40
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 3.485907554626465,
      "learning_rate": 4.824561403508772e-05,
      "loss": 1.9357,
      "step": 50
    },
    {
      "epoch": 0.12631578947368421,
      "grad_norm": 3.4603524208068848,
      "learning_rate": 4.789473684210526e-05,
      "loss": 1.978,
      "step": 60
    },
    {
      "epoch": 0.14736842105263157,
      "grad_norm": 2.2628092765808105,
      "learning_rate": 4.754385964912281e-05,
      "loss": 2.0195,
      "step": 70
    },
    {
      "epoch": 0.16842105263157894,
      "grad_norm": 6.4086456298828125,
      "learning_rate": 4.719298245614036e-05,
      "loss": 1.8846,
      "step": 80
    },
    {
      "epoch": 0.18947368421052632,
      "grad_norm": 3.6985104084014893,
      "learning_rate": 4.68421052631579e-05,
      "loss": 2.0152,
      "step": 90
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 3.600857973098755,
      "learning_rate": 4.649122807017544e-05,
      "loss": 2.0072,
      "step": 100
    },
    {
      "epoch": 0.23157894736842105,
      "grad_norm": 5.303938388824463,
      "learning_rate": 4.6140350877192985e-05,
      "loss": 1.944,
      "step": 110
    },
    {
      "epoch": 0.25263157894736843,
      "grad_norm": 2.4260289669036865,
      "learning_rate": 4.5789473684210527e-05,
      "loss": 1.8751,
      "step": 120
    },
    {
      "epoch": 0.2736842105263158,
      "grad_norm": 5.124927043914795,
      "learning_rate": 4.5438596491228075e-05,
      "loss": 1.8862,
      "step": 130
    },
    {
      "epoch": 0.29473684210526313,
      "grad_norm": 4.567234992980957,
      "learning_rate": 4.508771929824562e-05,
      "loss": 1.8835,
      "step": 140
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 3.8396599292755127,
      "learning_rate": 4.473684210526316e-05,
      "loss": 1.8823,
      "step": 150
    },
    {
      "epoch": 0.3368421052631579,
      "grad_norm": 5.085453510284424,
      "learning_rate": 4.43859649122807e-05,
      "loss": 1.8615,
      "step": 160
    },
    {
      "epoch": 0.35789473684210527,
      "grad_norm": 3.0522351264953613,
      "learning_rate": 4.403508771929824e-05,
      "loss": 1.8815,
      "step": 170
    },
    {
      "epoch": 0.37894736842105264,
      "grad_norm": 9.399946212768555,
      "learning_rate": 4.368421052631579e-05,
      "loss": 1.7169,
      "step": 180
    },
    {
      "epoch": 0.4,
      "grad_norm": 7.576830863952637,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 1.805,
      "step": 190
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 6.16895055770874,
      "learning_rate": 4.298245614035088e-05,
      "loss": 1.9679,
      "step": 200
    },
    {
      "epoch": 0.4421052631578947,
      "grad_norm": 4.1898298263549805,
      "learning_rate": 4.2631578947368425e-05,
      "loss": 1.9676,
      "step": 210
    },
    {
      "epoch": 0.4631578947368421,
      "grad_norm": 4.826864242553711,
      "learning_rate": 4.228070175438597e-05,
      "loss": 1.8483,
      "step": 220
    },
    {
      "epoch": 0.4842105263157895,
      "grad_norm": 4.175567150115967,
      "learning_rate": 4.1929824561403516e-05,
      "loss": 1.6844,
      "step": 230
    },
    {
      "epoch": 0.5052631578947369,
      "grad_norm": 50.30054473876953,
      "learning_rate": 4.157894736842106e-05,
      "loss": 1.8268,
      "step": 240
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 4.76958703994751,
      "learning_rate": 4.12280701754386e-05,
      "loss": 1.8678,
      "step": 250
    },
    {
      "epoch": 0.5473684210526316,
      "grad_norm": 2.4609193801879883,
      "learning_rate": 4.087719298245614e-05,
      "loss": 1.7786,
      "step": 260
    },
    {
      "epoch": 0.5684210526315789,
      "grad_norm": 6.661571025848389,
      "learning_rate": 4.0526315789473684e-05,
      "loss": 1.9218,
      "step": 270
    },
    {
      "epoch": 0.5894736842105263,
      "grad_norm": 7.633248805999756,
      "learning_rate": 4.017543859649123e-05,
      "loss": 1.7775,
      "step": 280
    },
    {
      "epoch": 0.6105263157894737,
      "grad_norm": 3.5345542430877686,
      "learning_rate": 3.9824561403508774e-05,
      "loss": 1.8544,
      "step": 290
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 8.888227462768555,
      "learning_rate": 3.9473684210526316e-05,
      "loss": 1.8042,
      "step": 300
    },
    {
      "epoch": 0.6526315789473685,
      "grad_norm": 3.2630300521850586,
      "learning_rate": 3.912280701754386e-05,
      "loss": 1.7824,
      "step": 310
    },
    {
      "epoch": 0.6736842105263158,
      "grad_norm": 4.041922569274902,
      "learning_rate": 3.877192982456141e-05,
      "loss": 1.7601,
      "step": 320
    },
    {
      "epoch": 0.6947368421052632,
      "grad_norm": 5.106021404266357,
      "learning_rate": 3.842105263157895e-05,
      "loss": 1.6669,
      "step": 330
    },
    {
      "epoch": 0.7157894736842105,
      "grad_norm": 5.798254013061523,
      "learning_rate": 3.80701754385965e-05,
      "loss": 1.5486,
      "step": 340
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 4.517367839813232,
      "learning_rate": 3.771929824561404e-05,
      "loss": 1.7815,
      "step": 350
    },
    {
      "epoch": 0.7578947368421053,
      "grad_norm": 8.817208290100098,
      "learning_rate": 3.736842105263158e-05,
      "loss": 1.7061,
      "step": 360
    },
    {
      "epoch": 0.7789473684210526,
      "grad_norm": 3.5075013637542725,
      "learning_rate": 3.7017543859649124e-05,
      "loss": 1.7548,
      "step": 370
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.560654640197754,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 1.7303,
      "step": 380
    },
    {
      "epoch": 0.8210526315789474,
      "grad_norm": 5.060314178466797,
      "learning_rate": 3.6315789473684214e-05,
      "loss": 1.6921,
      "step": 390
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 3.6418333053588867,
      "learning_rate": 3.5964912280701756e-05,
      "loss": 1.6425,
      "step": 400
    },
    {
      "epoch": 0.8631578947368421,
      "grad_norm": 6.825775146484375,
      "learning_rate": 3.56140350877193e-05,
      "loss": 1.6724,
      "step": 410
    },
    {
      "epoch": 0.8842105263157894,
      "grad_norm": 4.231351852416992,
      "learning_rate": 3.526315789473684e-05,
      "loss": 1.6621,
      "step": 420
    },
    {
      "epoch": 0.9052631578947369,
      "grad_norm": 3.91825532913208,
      "learning_rate": 3.491228070175438e-05,
      "loss": 1.6885,
      "step": 430
    },
    {
      "epoch": 0.9263157894736842,
      "grad_norm": 6.195812225341797,
      "learning_rate": 3.456140350877193e-05,
      "loss": 1.7855,
      "step": 440
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 6.440107822418213,
      "learning_rate": 3.421052631578947e-05,
      "loss": 1.7232,
      "step": 450
    },
    {
      "epoch": 0.968421052631579,
      "grad_norm": 7.648545265197754,
      "learning_rate": 3.385964912280702e-05,
      "loss": 1.6581,
      "step": 460
    },
    {
      "epoch": 0.9894736842105263,
      "grad_norm": 4.875276565551758,
      "learning_rate": 3.3508771929824564e-05,
      "loss": 1.606,
      "step": 470
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.77770926188814,
      "eval_loss": 1.5709123611450195,
      "eval_precision": 0.9613278415043608,
      "eval_recall": 0.6656698905977485,
      "eval_runtime": 5.5601,
      "eval_samples_per_second": 455.028,
      "eval_steps_per_second": 28.597,
      "step": 475
    },
    {
      "epoch": 1.0105263157894737,
      "grad_norm": 6.205299377441406,
      "learning_rate": 3.3157894736842106e-05,
      "loss": 1.6485,
      "step": 480
    },
    {
      "epoch": 1.0315789473684212,
      "grad_norm": 6.362636089324951,
      "learning_rate": 3.2807017543859655e-05,
      "loss": 1.4504,
      "step": 490
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 4.532998085021973,
      "learning_rate": 3.24561403508772e-05,
      "loss": 1.6288,
      "step": 500
    },
    {
      "epoch": 1.0736842105263158,
      "grad_norm": 6.386768341064453,
      "learning_rate": 3.210526315789474e-05,
      "loss": 1.4166,
      "step": 510
    },
    {
      "epoch": 1.0947368421052632,
      "grad_norm": 7.88625431060791,
      "learning_rate": 3.175438596491228e-05,
      "loss": 1.508,
      "step": 520
    },
    {
      "epoch": 1.1157894736842104,
      "grad_norm": 4.330288887023926,
      "learning_rate": 3.140350877192982e-05,
      "loss": 1.5834,
      "step": 530
    },
    {
      "epoch": 1.1368421052631579,
      "grad_norm": 5.630032062530518,
      "learning_rate": 3.105263157894737e-05,
      "loss": 1.5262,
      "step": 540
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 5.412298202514648,
      "learning_rate": 3.0701754385964913e-05,
      "loss": 1.5508,
      "step": 550
    },
    {
      "epoch": 1.1789473684210527,
      "grad_norm": 6.226003646850586,
      "learning_rate": 3.035087719298246e-05,
      "loss": 1.4695,
      "step": 560
    },
    {
      "epoch": 1.2,
      "grad_norm": 8.92876148223877,
      "learning_rate": 3e-05,
      "loss": 1.3402,
      "step": 570
    },
    {
      "epoch": 1.2210526315789474,
      "grad_norm": 14.417007446289062,
      "learning_rate": 2.9649122807017543e-05,
      "loss": 1.5202,
      "step": 580
    },
    {
      "epoch": 1.2421052631578948,
      "grad_norm": 9.229269981384277,
      "learning_rate": 2.929824561403509e-05,
      "loss": 1.5278,
      "step": 590
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 14.752531051635742,
      "learning_rate": 2.8947368421052634e-05,
      "loss": 1.4435,
      "step": 600
    },
    {
      "epoch": 1.2842105263157895,
      "grad_norm": 4.9949631690979,
      "learning_rate": 2.8596491228070175e-05,
      "loss": 1.5446,
      "step": 610
    },
    {
      "epoch": 1.305263157894737,
      "grad_norm": 6.277518272399902,
      "learning_rate": 2.824561403508772e-05,
      "loss": 1.471,
      "step": 620
    },
    {
      "epoch": 1.3263157894736843,
      "grad_norm": 8.325213432312012,
      "learning_rate": 2.7894736842105263e-05,
      "loss": 1.4469,
      "step": 630
    },
    {
      "epoch": 1.3473684210526315,
      "grad_norm": 10.914318084716797,
      "learning_rate": 2.754385964912281e-05,
      "loss": 1.4713,
      "step": 640
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 7.29030179977417,
      "learning_rate": 2.7192982456140354e-05,
      "loss": 1.4813,
      "step": 650
    },
    {
      "epoch": 1.3894736842105262,
      "grad_norm": 7.136543273925781,
      "learning_rate": 2.6842105263157896e-05,
      "loss": 1.2818,
      "step": 660
    },
    {
      "epoch": 1.4105263157894736,
      "grad_norm": 4.530279636383057,
      "learning_rate": 2.6491228070175438e-05,
      "loss": 1.4542,
      "step": 670
    },
    {
      "epoch": 1.431578947368421,
      "grad_norm": 5.3207106590271,
      "learning_rate": 2.6140350877192983e-05,
      "loss": 1.4514,
      "step": 680
    },
    {
      "epoch": 1.4526315789473685,
      "grad_norm": 5.608826637268066,
      "learning_rate": 2.578947368421053e-05,
      "loss": 1.48,
      "step": 690
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 7.804148197174072,
      "learning_rate": 2.5438596491228074e-05,
      "loss": 1.6093,
      "step": 700
    },
    {
      "epoch": 1.4947368421052631,
      "grad_norm": 7.413520336151123,
      "learning_rate": 2.5087719298245616e-05,
      "loss": 1.5729,
      "step": 710
    },
    {
      "epoch": 1.5157894736842106,
      "grad_norm": 6.506826400756836,
      "learning_rate": 2.4736842105263158e-05,
      "loss": 1.4626,
      "step": 720
    },
    {
      "epoch": 1.5368421052631578,
      "grad_norm": 5.5911760330200195,
      "learning_rate": 2.4385964912280703e-05,
      "loss": 1.3858,
      "step": 730
    },
    {
      "epoch": 1.5578947368421052,
      "grad_norm": 5.847962379455566,
      "learning_rate": 2.4035087719298245e-05,
      "loss": 1.3561,
      "step": 740
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 7.869871139526367,
      "learning_rate": 2.368421052631579e-05,
      "loss": 1.4771,
      "step": 750
    },
    {
      "epoch": 1.6,
      "grad_norm": 7.0913872718811035,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 1.3774,
      "step": 760
    },
    {
      "epoch": 1.6210526315789475,
      "grad_norm": 5.927513599395752,
      "learning_rate": 2.2982456140350878e-05,
      "loss": 1.379,
      "step": 770
    },
    {
      "epoch": 1.6421052631578947,
      "grad_norm": 8.764595985412598,
      "learning_rate": 2.2631578947368423e-05,
      "loss": 1.366,
      "step": 780
    },
    {
      "epoch": 1.663157894736842,
      "grad_norm": 5.934321880340576,
      "learning_rate": 2.2280701754385965e-05,
      "loss": 1.5078,
      "step": 790
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 5.785583019256592,
      "learning_rate": 2.1929824561403507e-05,
      "loss": 1.2519,
      "step": 800
    },
    {
      "epoch": 1.7052631578947368,
      "grad_norm": 8.224013328552246,
      "learning_rate": 2.1578947368421053e-05,
      "loss": 1.3204,
      "step": 810
    },
    {
      "epoch": 1.7263157894736842,
      "grad_norm": 6.973034858703613,
      "learning_rate": 2.1228070175438598e-05,
      "loss": 1.4254,
      "step": 820
    },
    {
      "epoch": 1.7473684210526317,
      "grad_norm": 6.730278491973877,
      "learning_rate": 2.0877192982456143e-05,
      "loss": 1.3437,
      "step": 830
    },
    {
      "epoch": 1.768421052631579,
      "grad_norm": 7.096921920776367,
      "learning_rate": 2.0526315789473685e-05,
      "loss": 1.3292,
      "step": 840
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 14.324106216430664,
      "learning_rate": 2.0175438596491227e-05,
      "loss": 1.3548,
      "step": 850
    },
    {
      "epoch": 1.8105263157894735,
      "grad_norm": 10.569236755371094,
      "learning_rate": 1.9824561403508773e-05,
      "loss": 1.4567,
      "step": 860
    },
    {
      "epoch": 1.831578947368421,
      "grad_norm": 4.911665439605713,
      "learning_rate": 1.9473684210526315e-05,
      "loss": 1.3698,
      "step": 870
    },
    {
      "epoch": 1.8526315789473684,
      "grad_norm": 9.735340118408203,
      "learning_rate": 1.9122807017543863e-05,
      "loss": 1.3067,
      "step": 880
    },
    {
      "epoch": 1.8736842105263158,
      "grad_norm": 13.401127815246582,
      "learning_rate": 1.8771929824561405e-05,
      "loss": 1.3767,
      "step": 890
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 6.531028747558594,
      "learning_rate": 1.8421052631578947e-05,
      "loss": 1.3899,
      "step": 900
    },
    {
      "epoch": 1.9157894736842105,
      "grad_norm": 11.35387134552002,
      "learning_rate": 1.8070175438596493e-05,
      "loss": 1.266,
      "step": 910
    },
    {
      "epoch": 1.936842105263158,
      "grad_norm": 6.153669834136963,
      "learning_rate": 1.7719298245614035e-05,
      "loss": 1.4498,
      "step": 920
    },
    {
      "epoch": 1.9578947368421051,
      "grad_norm": 6.351660251617432,
      "learning_rate": 1.736842105263158e-05,
      "loss": 1.3821,
      "step": 930
    },
    {
      "epoch": 1.9789473684210526,
      "grad_norm": 7.3589277267456055,
      "learning_rate": 1.7017543859649125e-05,
      "loss": 1.4136,
      "step": 940
    },
    {
      "epoch": 2.0,
      "grad_norm": 20.23267936706543,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.2192,
      "step": 950
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.8622814591834831,
      "eval_loss": 1.4058247804641724,
      "eval_precision": 0.9583223863141668,
      "eval_recall": 0.7936927223719676,
      "eval_runtime": 5.6702,
      "eval_samples_per_second": 446.196,
      "eval_steps_per_second": 28.042,
      "step": 950
    },
    {
      "epoch": 2.0210526315789474,
      "grad_norm": 6.810110092163086,
      "learning_rate": 1.6315789473684213e-05,
      "loss": 1.2136,
      "step": 960
    },
    {
      "epoch": 2.042105263157895,
      "grad_norm": 8.315711975097656,
      "learning_rate": 1.5964912280701755e-05,
      "loss": 1.2753,
      "step": 970
    },
    {
      "epoch": 2.0631578947368423,
      "grad_norm": 6.401732921600342,
      "learning_rate": 1.56140350877193e-05,
      "loss": 1.3555,
      "step": 980
    },
    {
      "epoch": 2.0842105263157893,
      "grad_norm": 4.8918609619140625,
      "learning_rate": 1.5263157894736842e-05,
      "loss": 1.1127,
      "step": 990
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 17.428367614746094,
      "learning_rate": 1.4912280701754386e-05,
      "loss": 1.1648,
      "step": 1000
    },
    {
      "epoch": 2.126315789473684,
      "grad_norm": 7.081417083740234,
      "learning_rate": 1.4561403508771931e-05,
      "loss": 1.1804,
      "step": 1010
    },
    {
      "epoch": 2.1473684210526316,
      "grad_norm": 11.758066177368164,
      "learning_rate": 1.4210526315789475e-05,
      "loss": 1.1251,
      "step": 1020
    },
    {
      "epoch": 2.168421052631579,
      "grad_norm": 7.418376445770264,
      "learning_rate": 1.3859649122807017e-05,
      "loss": 1.0786,
      "step": 1030
    },
    {
      "epoch": 2.1894736842105265,
      "grad_norm": 11.895968437194824,
      "learning_rate": 1.3508771929824562e-05,
      "loss": 1.1816,
      "step": 1040
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 17.72893524169922,
      "learning_rate": 1.3157894736842106e-05,
      "loss": 0.9811,
      "step": 1050
    },
    {
      "epoch": 2.231578947368421,
      "grad_norm": 6.749232292175293,
      "learning_rate": 1.2807017543859651e-05,
      "loss": 1.0839,
      "step": 1060
    },
    {
      "epoch": 2.2526315789473683,
      "grad_norm": 7.2694902420043945,
      "learning_rate": 1.2456140350877193e-05,
      "loss": 1.212,
      "step": 1070
    },
    {
      "epoch": 2.2736842105263158,
      "grad_norm": 9.707374572753906,
      "learning_rate": 1.2105263157894737e-05,
      "loss": 1.294,
      "step": 1080
    },
    {
      "epoch": 2.294736842105263,
      "grad_norm": 9.135324478149414,
      "learning_rate": 1.1754385964912282e-05,
      "loss": 1.3034,
      "step": 1090
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 6.745755195617676,
      "learning_rate": 1.1403508771929824e-05,
      "loss": 1.1922,
      "step": 1100
    },
    {
      "epoch": 2.336842105263158,
      "grad_norm": 7.984361171722412,
      "learning_rate": 1.1052631578947368e-05,
      "loss": 1.1384,
      "step": 1110
    },
    {
      "epoch": 2.3578947368421055,
      "grad_norm": 6.229238510131836,
      "learning_rate": 1.0701754385964913e-05,
      "loss": 1.0308,
      "step": 1120
    },
    {
      "epoch": 2.3789473684210525,
      "grad_norm": 7.379026889801025,
      "learning_rate": 1.0350877192982457e-05,
      "loss": 1.1408,
      "step": 1130
    },
    {
      "epoch": 2.4,
      "grad_norm": 8.906984329223633,
      "learning_rate": 1e-05,
      "loss": 1.053,
      "step": 1140
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 9.795351028442383,
      "learning_rate": 9.649122807017545e-06,
      "loss": 1.1074,
      "step": 1150
    },
    {
      "epoch": 2.442105263157895,
      "grad_norm": 11.981369972229004,
      "learning_rate": 9.298245614035088e-06,
      "loss": 1.0795,
      "step": 1160
    },
    {
      "epoch": 2.463157894736842,
      "grad_norm": 9.338190078735352,
      "learning_rate": 8.947368421052632e-06,
      "loss": 1.1747,
      "step": 1170
    },
    {
      "epoch": 2.4842105263157896,
      "grad_norm": 8.272905349731445,
      "learning_rate": 8.596491228070176e-06,
      "loss": 1.0384,
      "step": 1180
    },
    {
      "epoch": 2.5052631578947366,
      "grad_norm": 9.360929489135742,
      "learning_rate": 8.245614035087721e-06,
      "loss": 1.2247,
      "step": 1190
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 8.819818496704102,
      "learning_rate": 7.894736842105263e-06,
      "loss": 1.1443,
      "step": 1200
    },
    {
      "epoch": 2.5473684210526315,
      "grad_norm": 6.751266956329346,
      "learning_rate": 7.5438596491228074e-06,
      "loss": 1.0673,
      "step": 1210
    },
    {
      "epoch": 2.568421052631579,
      "grad_norm": 7.968820095062256,
      "learning_rate": 7.192982456140351e-06,
      "loss": 1.0108,
      "step": 1220
    },
    {
      "epoch": 2.5894736842105264,
      "grad_norm": 11.274651527404785,
      "learning_rate": 6.842105263157896e-06,
      "loss": 1.1944,
      "step": 1230
    },
    {
      "epoch": 2.610526315789474,
      "grad_norm": 6.049420356750488,
      "learning_rate": 6.4912280701754385e-06,
      "loss": 1.0695,
      "step": 1240
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 6.541892051696777,
      "learning_rate": 6.140350877192982e-06,
      "loss": 1.0927,
      "step": 1250
    },
    {
      "epoch": 2.6526315789473687,
      "grad_norm": 12.976258277893066,
      "learning_rate": 5.789473684210527e-06,
      "loss": 1.1898,
      "step": 1260
    },
    {
      "epoch": 2.6736842105263157,
      "grad_norm": 9.840328216552734,
      "learning_rate": 5.43859649122807e-06,
      "loss": 1.0759,
      "step": 1270
    },
    {
      "epoch": 2.694736842105263,
      "grad_norm": 5.700351715087891,
      "learning_rate": 5.087719298245614e-06,
      "loss": 1.1194,
      "step": 1280
    },
    {
      "epoch": 2.7157894736842105,
      "grad_norm": 10.095270156860352,
      "learning_rate": 4.736842105263159e-06,
      "loss": 1.0483,
      "step": 1290
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 9.523652076721191,
      "learning_rate": 4.3859649122807014e-06,
      "loss": 1.2132,
      "step": 1300
    },
    {
      "epoch": 2.7578947368421054,
      "grad_norm": 8.109882354736328,
      "learning_rate": 4.035087719298246e-06,
      "loss": 1.1453,
      "step": 1310
    },
    {
      "epoch": 2.7789473684210524,
      "grad_norm": 7.110302925109863,
      "learning_rate": 3.6842105263157892e-06,
      "loss": 1.1945,
      "step": 1320
    },
    {
      "epoch": 2.8,
      "grad_norm": 8.388519287109375,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.2014,
      "step": 1330
    },
    {
      "epoch": 2.8210526315789473,
      "grad_norm": 9.05810546875,
      "learning_rate": 2.9824561403508774e-06,
      "loss": 1.0423,
      "step": 1340
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 8.286636352539062,
      "learning_rate": 2.631578947368421e-06,
      "loss": 1.0719,
      "step": 1350
    },
    {
      "epoch": 2.863157894736842,
      "grad_norm": 7.016646385192871,
      "learning_rate": 2.2807017543859652e-06,
      "loss": 1.1683,
      "step": 1360
    },
    {
      "epoch": 2.8842105263157896,
      "grad_norm": 7.092879295349121,
      "learning_rate": 1.929824561403509e-06,
      "loss": 1.2375,
      "step": 1370
    },
    {
      "epoch": 2.905263157894737,
      "grad_norm": 11.897939682006836,
      "learning_rate": 1.5789473684210528e-06,
      "loss": 1.1083,
      "step": 1380
    },
    {
      "epoch": 2.9263157894736844,
      "grad_norm": 10.802034378051758,
      "learning_rate": 1.2280701754385965e-06,
      "loss": 1.0131,
      "step": 1390
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 8.781696319580078,
      "learning_rate": 8.771929824561404e-07,
      "loss": 1.0041,
      "step": 1400
    },
    {
      "epoch": 2.968421052631579,
      "grad_norm": 8.11404800415039,
      "learning_rate": 5.263157894736843e-07,
      "loss": 1.115,
      "step": 1410
    },
    {
      "epoch": 2.9894736842105263,
      "grad_norm": 7.453617572784424,
      "learning_rate": 1.7543859649122808e-07,
      "loss": 1.0868,
      "step": 1420
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.8267162495266872,
      "eval_loss": 1.3078809976577759,
      "eval_precision": 0.9616896418590491,
      "eval_recall": 0.7383034723323291,
      "eval_runtime": 5.7022,
      "eval_samples_per_second": 443.691,
      "eval_steps_per_second": 27.884,
      "step": 1425
    }
  ],
  "logging_steps": 10,
  "max_steps": 1425,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1487133952238592.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
