{
  "best_metric": 0.27233844000311064,
  "best_model_checkpoint": "./results\\checkpoint-1443",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1443,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02079002079002079,
      "grad_norm": 2.193981647491455,
      "learning_rate": 1.9861399861399863e-05,
      "loss": 2.2354,
      "step": 10
    },
    {
      "epoch": 0.04158004158004158,
      "grad_norm": 2.3562519550323486,
      "learning_rate": 1.9722799722799724e-05,
      "loss": 2.1944,
      "step": 20
    },
    {
      "epoch": 0.062370062370062374,
      "grad_norm": 2.9385111331939697,
      "learning_rate": 1.9584199584199584e-05,
      "loss": 2.1931,
      "step": 30
    },
    {
      "epoch": 0.08316008316008316,
      "grad_norm": 2.0042502880096436,
      "learning_rate": 1.944559944559945e-05,
      "loss": 2.1688,
      "step": 40
    },
    {
      "epoch": 0.10395010395010396,
      "grad_norm": 3.6055994033813477,
      "learning_rate": 1.9306999306999306e-05,
      "loss": 2.1472,
      "step": 50
    },
    {
      "epoch": 0.12474012474012475,
      "grad_norm": 2.2963438034057617,
      "learning_rate": 1.916839916839917e-05,
      "loss": 2.1567,
      "step": 60
    },
    {
      "epoch": 0.14553014553014554,
      "grad_norm": 3.430410623550415,
      "learning_rate": 1.902979902979903e-05,
      "loss": 2.1247,
      "step": 70
    },
    {
      "epoch": 0.16632016632016633,
      "grad_norm": 2.3013744354248047,
      "learning_rate": 1.8891198891198893e-05,
      "loss": 2.1378,
      "step": 80
    },
    {
      "epoch": 0.18711018711018712,
      "grad_norm": 2.6689610481262207,
      "learning_rate": 1.8752598752598754e-05,
      "loss": 2.1146,
      "step": 90
    },
    {
      "epoch": 0.2079002079002079,
      "grad_norm": 3.50150990486145,
      "learning_rate": 1.8613998613998615e-05,
      "loss": 2.1266,
      "step": 100
    },
    {
      "epoch": 0.2286902286902287,
      "grad_norm": 4.909278869628906,
      "learning_rate": 1.8475398475398475e-05,
      "loss": 2.0788,
      "step": 110
    },
    {
      "epoch": 0.2494802494802495,
      "grad_norm": 2.8818907737731934,
      "learning_rate": 1.833679833679834e-05,
      "loss": 2.0388,
      "step": 120
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 3.83284330368042,
      "learning_rate": 1.81981981981982e-05,
      "loss": 2.0825,
      "step": 130
    },
    {
      "epoch": 0.2910602910602911,
      "grad_norm": 4.782238960266113,
      "learning_rate": 1.8059598059598062e-05,
      "loss": 1.9977,
      "step": 140
    },
    {
      "epoch": 0.31185031185031187,
      "grad_norm": 2.8297202587127686,
      "learning_rate": 1.7920997920997923e-05,
      "loss": 2.0264,
      "step": 150
    },
    {
      "epoch": 0.33264033264033266,
      "grad_norm": 6.495297908782959,
      "learning_rate": 1.7782397782397784e-05,
      "loss": 2.0134,
      "step": 160
    },
    {
      "epoch": 0.35343035343035345,
      "grad_norm": 2.896925449371338,
      "learning_rate": 1.7643797643797645e-05,
      "loss": 2.0338,
      "step": 170
    },
    {
      "epoch": 0.37422037422037424,
      "grad_norm": 6.1751389503479,
      "learning_rate": 1.7505197505197506e-05,
      "loss": 1.9978,
      "step": 180
    },
    {
      "epoch": 0.39501039501039503,
      "grad_norm": 10.244867324829102,
      "learning_rate": 1.736659736659737e-05,
      "loss": 1.9901,
      "step": 190
    },
    {
      "epoch": 0.4158004158004158,
      "grad_norm": 2.703089714050293,
      "learning_rate": 1.7227997227997227e-05,
      "loss": 1.9918,
      "step": 200
    },
    {
      "epoch": 0.4365904365904366,
      "grad_norm": 5.285416603088379,
      "learning_rate": 1.7089397089397092e-05,
      "loss": 1.9886,
      "step": 210
    },
    {
      "epoch": 0.4573804573804574,
      "grad_norm": 21.265338897705078,
      "learning_rate": 1.6950796950796953e-05,
      "loss": 2.0769,
      "step": 220
    },
    {
      "epoch": 0.4781704781704782,
      "grad_norm": 3.2337706089019775,
      "learning_rate": 1.6812196812196814e-05,
      "loss": 2.0126,
      "step": 230
    },
    {
      "epoch": 0.498960498960499,
      "grad_norm": 3.1315243244171143,
      "learning_rate": 1.6673596673596675e-05,
      "loss": 2.0562,
      "step": 240
    },
    {
      "epoch": 0.5197505197505198,
      "grad_norm": 6.302496910095215,
      "learning_rate": 1.6534996534996536e-05,
      "loss": 1.9966,
      "step": 250
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 2.916961193084717,
      "learning_rate": 1.6396396396396396e-05,
      "loss": 1.9706,
      "step": 260
    },
    {
      "epoch": 0.5613305613305614,
      "grad_norm": 2.677245616912842,
      "learning_rate": 1.6257796257796257e-05,
      "loss": 1.9628,
      "step": 270
    },
    {
      "epoch": 0.5821205821205822,
      "grad_norm": 12.698385238647461,
      "learning_rate": 1.6119196119196122e-05,
      "loss": 1.9659,
      "step": 280
    },
    {
      "epoch": 0.6029106029106029,
      "grad_norm": 3.4229378700256348,
      "learning_rate": 1.598059598059598e-05,
      "loss": 1.9645,
      "step": 290
    },
    {
      "epoch": 0.6237006237006237,
      "grad_norm": 2.396493434906006,
      "learning_rate": 1.5841995841995844e-05,
      "loss": 1.9253,
      "step": 300
    },
    {
      "epoch": 0.6444906444906445,
      "grad_norm": 4.501940727233887,
      "learning_rate": 1.5703395703395705e-05,
      "loss": 1.9072,
      "step": 310
    },
    {
      "epoch": 0.6652806652806653,
      "grad_norm": 3.3508758544921875,
      "learning_rate": 1.5564795564795566e-05,
      "loss": 1.9206,
      "step": 320
    },
    {
      "epoch": 0.6860706860706861,
      "grad_norm": 2.4396395683288574,
      "learning_rate": 1.5426195426195427e-05,
      "loss": 1.9932,
      "step": 330
    },
    {
      "epoch": 0.7068607068607069,
      "grad_norm": 3.9159319400787354,
      "learning_rate": 1.528759528759529e-05,
      "loss": 1.9642,
      "step": 340
    },
    {
      "epoch": 0.7276507276507277,
      "grad_norm": 8.792511940002441,
      "learning_rate": 1.5148995148995148e-05,
      "loss": 1.8964,
      "step": 350
    },
    {
      "epoch": 0.7484407484407485,
      "grad_norm": 16.357419967651367,
      "learning_rate": 1.5010395010395011e-05,
      "loss": 1.9332,
      "step": 360
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 3.591456413269043,
      "learning_rate": 1.4871794871794874e-05,
      "loss": 1.8964,
      "step": 370
    },
    {
      "epoch": 0.7900207900207901,
      "grad_norm": 42.21683883666992,
      "learning_rate": 1.4733194733194733e-05,
      "loss": 1.8635,
      "step": 380
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 2.9866528511047363,
      "learning_rate": 1.4594594594594596e-05,
      "loss": 1.9182,
      "step": 390
    },
    {
      "epoch": 0.8316008316008316,
      "grad_norm": 3.628279209136963,
      "learning_rate": 1.4455994455994458e-05,
      "loss": 1.9247,
      "step": 400
    },
    {
      "epoch": 0.8523908523908524,
      "grad_norm": 4.012393951416016,
      "learning_rate": 1.4317394317394318e-05,
      "loss": 1.8845,
      "step": 410
    },
    {
      "epoch": 0.8731808731808732,
      "grad_norm": 3.888122081756592,
      "learning_rate": 1.417879417879418e-05,
      "loss": 1.9142,
      "step": 420
    },
    {
      "epoch": 0.893970893970894,
      "grad_norm": 2.2179365158081055,
      "learning_rate": 1.4040194040194041e-05,
      "loss": 1.8853,
      "step": 430
    },
    {
      "epoch": 0.9147609147609148,
      "grad_norm": 2.172210454940796,
      "learning_rate": 1.3901593901593902e-05,
      "loss": 1.9083,
      "step": 440
    },
    {
      "epoch": 0.9355509355509356,
      "grad_norm": 3.477043628692627,
      "learning_rate": 1.3762993762993763e-05,
      "loss": 1.9277,
      "step": 450
    },
    {
      "epoch": 0.9563409563409564,
      "grad_norm": 3.401667356491089,
      "learning_rate": 1.3624393624393626e-05,
      "loss": 1.9088,
      "step": 460
    },
    {
      "epoch": 0.9771309771309772,
      "grad_norm": 5.604522228240967,
      "learning_rate": 1.3485793485793487e-05,
      "loss": 1.9691,
      "step": 470
    },
    {
      "epoch": 0.997920997920998,
      "grad_norm": 4.4370646476745605,
      "learning_rate": 1.3347193347193348e-05,
      "loss": 1.8805,
      "step": 480
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.21922388988257252,
      "eval_f1": 0.21922388988257252,
      "eval_loss": 1.8840360641479492,
      "eval_precision": 0.21922388988257252,
      "eval_recall": 0.21922388988257252,
      "eval_runtime": 19.6451,
      "eval_samples_per_second": 130.414,
      "eval_steps_per_second": 8.195,
      "step": 481
    },
    {
      "epoch": 1.0187110187110187,
      "grad_norm": 2.8240702152252197,
      "learning_rate": 1.320859320859321e-05,
      "loss": 1.9042,
      "step": 490
    },
    {
      "epoch": 1.0395010395010396,
      "grad_norm": 3.7947866916656494,
      "learning_rate": 1.3069993069993073e-05,
      "loss": 1.9566,
      "step": 500
    },
    {
      "epoch": 1.0602910602910602,
      "grad_norm": 4.822104454040527,
      "learning_rate": 1.2931392931392932e-05,
      "loss": 1.8195,
      "step": 510
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 3.1885199546813965,
      "learning_rate": 1.2792792792792795e-05,
      "loss": 1.8026,
      "step": 520
    },
    {
      "epoch": 1.1018711018711018,
      "grad_norm": 4.065295219421387,
      "learning_rate": 1.2654192654192656e-05,
      "loss": 1.8984,
      "step": 530
    },
    {
      "epoch": 1.1226611226611227,
      "grad_norm": 5.76627779006958,
      "learning_rate": 1.2515592515592517e-05,
      "loss": 1.8629,
      "step": 540
    },
    {
      "epoch": 1.1434511434511434,
      "grad_norm": 3.517155408859253,
      "learning_rate": 1.2376992376992378e-05,
      "loss": 1.8243,
      "step": 550
    },
    {
      "epoch": 1.1642411642411643,
      "grad_norm": 6.916651248931885,
      "learning_rate": 1.223839223839224e-05,
      "loss": 1.869,
      "step": 560
    },
    {
      "epoch": 1.185031185031185,
      "grad_norm": 4.980651378631592,
      "learning_rate": 1.2099792099792101e-05,
      "loss": 1.8588,
      "step": 570
    },
    {
      "epoch": 1.2058212058212059,
      "grad_norm": 3.9028780460357666,
      "learning_rate": 1.1961191961191962e-05,
      "loss": 1.8293,
      "step": 580
    },
    {
      "epoch": 1.2266112266112266,
      "grad_norm": 3.672093629837036,
      "learning_rate": 1.1822591822591825e-05,
      "loss": 1.853,
      "step": 590
    },
    {
      "epoch": 1.2474012474012475,
      "grad_norm": 3.665060043334961,
      "learning_rate": 1.1683991683991684e-05,
      "loss": 1.7797,
      "step": 600
    },
    {
      "epoch": 1.2681912681912682,
      "grad_norm": 4.0543999671936035,
      "learning_rate": 1.1545391545391547e-05,
      "loss": 1.8098,
      "step": 610
    },
    {
      "epoch": 1.288981288981289,
      "grad_norm": 12.277441024780273,
      "learning_rate": 1.140679140679141e-05,
      "loss": 1.9261,
      "step": 620
    },
    {
      "epoch": 1.3097713097713097,
      "grad_norm": 4.86537504196167,
      "learning_rate": 1.1268191268191269e-05,
      "loss": 1.8329,
      "step": 630
    },
    {
      "epoch": 1.3305613305613306,
      "grad_norm": 3.0764241218566895,
      "learning_rate": 1.1129591129591131e-05,
      "loss": 1.7662,
      "step": 640
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 4.382540702819824,
      "learning_rate": 1.0990990990990992e-05,
      "loss": 1.8568,
      "step": 650
    },
    {
      "epoch": 1.3721413721413722,
      "grad_norm": 8.56420612335205,
      "learning_rate": 1.0852390852390853e-05,
      "loss": 1.8304,
      "step": 660
    },
    {
      "epoch": 1.392931392931393,
      "grad_norm": 5.0628252029418945,
      "learning_rate": 1.0713790713790714e-05,
      "loss": 1.7756,
      "step": 670
    },
    {
      "epoch": 1.4137214137214138,
      "grad_norm": 5.7321553230285645,
      "learning_rate": 1.0575190575190577e-05,
      "loss": 1.7931,
      "step": 680
    },
    {
      "epoch": 1.4345114345114345,
      "grad_norm": 3.876833200454712,
      "learning_rate": 1.0436590436590438e-05,
      "loss": 1.7825,
      "step": 690
    },
    {
      "epoch": 1.4553014553014554,
      "grad_norm": 5.407190799713135,
      "learning_rate": 1.0297990297990299e-05,
      "loss": 1.8793,
      "step": 700
    },
    {
      "epoch": 1.476091476091476,
      "grad_norm": 3.7218680381774902,
      "learning_rate": 1.0159390159390161e-05,
      "loss": 1.8679,
      "step": 710
    },
    {
      "epoch": 1.496881496881497,
      "grad_norm": 3.6281304359436035,
      "learning_rate": 1.002079002079002e-05,
      "loss": 1.8648,
      "step": 720
    },
    {
      "epoch": 1.5176715176715176,
      "grad_norm": 5.731878757476807,
      "learning_rate": 9.882189882189883e-06,
      "loss": 1.9273,
      "step": 730
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 4.472407817840576,
      "learning_rate": 9.743589743589744e-06,
      "loss": 1.8317,
      "step": 740
    },
    {
      "epoch": 1.5592515592515592,
      "grad_norm": 2.954775333404541,
      "learning_rate": 9.604989604989607e-06,
      "loss": 1.7965,
      "step": 750
    },
    {
      "epoch": 1.5800415800415801,
      "grad_norm": 4.2166900634765625,
      "learning_rate": 9.466389466389468e-06,
      "loss": 1.7898,
      "step": 760
    },
    {
      "epoch": 1.6008316008316008,
      "grad_norm": 4.095693111419678,
      "learning_rate": 9.327789327789329e-06,
      "loss": 1.7153,
      "step": 770
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 5.928838729858398,
      "learning_rate": 9.189189189189191e-06,
      "loss": 1.8511,
      "step": 780
    },
    {
      "epoch": 1.6424116424116424,
      "grad_norm": 12.478046417236328,
      "learning_rate": 9.050589050589052e-06,
      "loss": 1.7669,
      "step": 790
    },
    {
      "epoch": 1.6632016632016633,
      "grad_norm": 7.092408180236816,
      "learning_rate": 8.911988911988913e-06,
      "loss": 1.7684,
      "step": 800
    },
    {
      "epoch": 1.683991683991684,
      "grad_norm": 11.537459373474121,
      "learning_rate": 8.773388773388774e-06,
      "loss": 1.8687,
      "step": 810
    },
    {
      "epoch": 1.7047817047817047,
      "grad_norm": 6.540332794189453,
      "learning_rate": 8.634788634788635e-06,
      "loss": 1.848,
      "step": 820
    },
    {
      "epoch": 1.7255717255717256,
      "grad_norm": 3.397763729095459,
      "learning_rate": 8.496188496188496e-06,
      "loss": 1.8438,
      "step": 830
    },
    {
      "epoch": 1.7463617463617465,
      "grad_norm": 2.8186259269714355,
      "learning_rate": 8.357588357588359e-06,
      "loss": 1.7542,
      "step": 840
    },
    {
      "epoch": 1.7671517671517671,
      "grad_norm": 5.429749965667725,
      "learning_rate": 8.21898821898822e-06,
      "loss": 1.7635,
      "step": 850
    },
    {
      "epoch": 1.7879417879417878,
      "grad_norm": 3.8898873329162598,
      "learning_rate": 8.08038808038808e-06,
      "loss": 1.7564,
      "step": 860
    },
    {
      "epoch": 1.8087318087318087,
      "grad_norm": 3.7188916206359863,
      "learning_rate": 7.941787941787943e-06,
      "loss": 1.7068,
      "step": 870
    },
    {
      "epoch": 1.8295218295218296,
      "grad_norm": 5.343271732330322,
      "learning_rate": 7.803187803187804e-06,
      "loss": 1.7726,
      "step": 880
    },
    {
      "epoch": 1.8503118503118503,
      "grad_norm": 4.07697868347168,
      "learning_rate": 7.664587664587665e-06,
      "loss": 1.8009,
      "step": 890
    },
    {
      "epoch": 1.871101871101871,
      "grad_norm": 5.428513050079346,
      "learning_rate": 7.525987525987527e-06,
      "loss": 1.7546,
      "step": 900
    },
    {
      "epoch": 1.8918918918918919,
      "grad_norm": 4.756373882293701,
      "learning_rate": 7.387387387387388e-06,
      "loss": 1.7718,
      "step": 910
    },
    {
      "epoch": 1.9126819126819128,
      "grad_norm": 6.120779991149902,
      "learning_rate": 7.248787248787249e-06,
      "loss": 1.7629,
      "step": 920
    },
    {
      "epoch": 1.9334719334719335,
      "grad_norm": 4.0605340003967285,
      "learning_rate": 7.1101871101871114e-06,
      "loss": 1.7376,
      "step": 930
    },
    {
      "epoch": 1.9542619542619541,
      "grad_norm": 6.905969619750977,
      "learning_rate": 6.971586971586972e-06,
      "loss": 1.7979,
      "step": 940
    },
    {
      "epoch": 1.975051975051975,
      "grad_norm": 5.49362325668335,
      "learning_rate": 6.832986832986833e-06,
      "loss": 1.7768,
      "step": 950
    },
    {
      "epoch": 1.995841995841996,
      "grad_norm": 10.177867889404297,
      "learning_rate": 6.694386694386695e-06,
      "loss": 1.8202,
      "step": 960
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.2489695932809705,
      "eval_f1": 0.2489695932809705,
      "eval_loss": 1.7734485864639282,
      "eval_precision": 0.2489695932809705,
      "eval_recall": 0.2489695932809705,
      "eval_runtime": 19.5284,
      "eval_samples_per_second": 131.193,
      "eval_steps_per_second": 8.244,
      "step": 962
    },
    {
      "epoch": 2.0166320166320166,
      "grad_norm": 3.5676467418670654,
      "learning_rate": 6.555786555786556e-06,
      "loss": 1.6771,
      "step": 970
    },
    {
      "epoch": 2.0374220374220373,
      "grad_norm": 5.537064075469971,
      "learning_rate": 6.417186417186417e-06,
      "loss": 1.7183,
      "step": 980
    },
    {
      "epoch": 2.0582120582120584,
      "grad_norm": 8.903094291687012,
      "learning_rate": 6.27858627858628e-06,
      "loss": 1.7294,
      "step": 990
    },
    {
      "epoch": 2.079002079002079,
      "grad_norm": 5.817540645599365,
      "learning_rate": 6.139986139986141e-06,
      "loss": 1.7481,
      "step": 1000
    },
    {
      "epoch": 2.0997920997921,
      "grad_norm": 5.086211681365967,
      "learning_rate": 6.0013860013860016e-06,
      "loss": 1.6723,
      "step": 1010
    },
    {
      "epoch": 2.1205821205821205,
      "grad_norm": 4.125220775604248,
      "learning_rate": 5.862785862785863e-06,
      "loss": 1.671,
      "step": 1020
    },
    {
      "epoch": 2.141372141372141,
      "grad_norm": 5.337635040283203,
      "learning_rate": 5.724185724185724e-06,
      "loss": 1.7723,
      "step": 1030
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 5.308239459991455,
      "learning_rate": 5.585585585585585e-06,
      "loss": 1.8071,
      "step": 1040
    },
    {
      "epoch": 2.182952182952183,
      "grad_norm": 5.772243976593018,
      "learning_rate": 5.446985446985448e-06,
      "loss": 1.7794,
      "step": 1050
    },
    {
      "epoch": 2.2037422037422036,
      "grad_norm": 6.349010467529297,
      "learning_rate": 5.308385308385309e-06,
      "loss": 1.7631,
      "step": 1060
    },
    {
      "epoch": 2.2245322245322248,
      "grad_norm": 3.7737739086151123,
      "learning_rate": 5.16978516978517e-06,
      "loss": 1.6105,
      "step": 1070
    },
    {
      "epoch": 2.2453222453222454,
      "grad_norm": 4.777219772338867,
      "learning_rate": 5.031185031185032e-06,
      "loss": 1.7578,
      "step": 1080
    },
    {
      "epoch": 2.266112266112266,
      "grad_norm": 4.474457263946533,
      "learning_rate": 4.8925848925848926e-06,
      "loss": 1.7173,
      "step": 1090
    },
    {
      "epoch": 2.286902286902287,
      "grad_norm": 3.9898571968078613,
      "learning_rate": 4.753984753984754e-06,
      "loss": 1.6954,
      "step": 1100
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 4.703271389007568,
      "learning_rate": 4.615384615384616e-06,
      "loss": 1.6654,
      "step": 1110
    },
    {
      "epoch": 2.3284823284823286,
      "grad_norm": 4.351897716522217,
      "learning_rate": 4.476784476784477e-06,
      "loss": 1.68,
      "step": 1120
    },
    {
      "epoch": 2.3492723492723493,
      "grad_norm": 4.345181465148926,
      "learning_rate": 4.338184338184339e-06,
      "loss": 1.6478,
      "step": 1130
    },
    {
      "epoch": 2.37006237006237,
      "grad_norm": 7.537665367126465,
      "learning_rate": 4.1995841995842e-06,
      "loss": 1.643,
      "step": 1140
    },
    {
      "epoch": 2.390852390852391,
      "grad_norm": 3.836217164993286,
      "learning_rate": 4.060984060984061e-06,
      "loss": 1.6787,
      "step": 1150
    },
    {
      "epoch": 2.4116424116424118,
      "grad_norm": 5.387747764587402,
      "learning_rate": 3.922383922383923e-06,
      "loss": 1.6917,
      "step": 1160
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 4.283420562744141,
      "learning_rate": 3.7837837837837844e-06,
      "loss": 1.7016,
      "step": 1170
    },
    {
      "epoch": 2.453222453222453,
      "grad_norm": 5.399159908294678,
      "learning_rate": 3.6451836451836453e-06,
      "loss": 1.6879,
      "step": 1180
    },
    {
      "epoch": 2.474012474012474,
      "grad_norm": 5.808358669281006,
      "learning_rate": 3.5065835065835067e-06,
      "loss": 1.7053,
      "step": 1190
    },
    {
      "epoch": 2.494802494802495,
      "grad_norm": 5.8515424728393555,
      "learning_rate": 3.3679833679833685e-06,
      "loss": 1.6889,
      "step": 1200
    },
    {
      "epoch": 2.5155925155925156,
      "grad_norm": 5.064820766448975,
      "learning_rate": 3.2293832293832295e-06,
      "loss": 1.7829,
      "step": 1210
    },
    {
      "epoch": 2.5363825363825363,
      "grad_norm": 5.375581741333008,
      "learning_rate": 3.090783090783091e-06,
      "loss": 1.7222,
      "step": 1220
    },
    {
      "epoch": 2.5571725571725574,
      "grad_norm": 4.661417484283447,
      "learning_rate": 2.9521829521829526e-06,
      "loss": 1.7542,
      "step": 1230
    },
    {
      "epoch": 2.577962577962578,
      "grad_norm": 4.381836414337158,
      "learning_rate": 2.8135828135828136e-06,
      "loss": 1.6481,
      "step": 1240
    },
    {
      "epoch": 2.598752598752599,
      "grad_norm": 4.2491936683654785,
      "learning_rate": 2.6749826749826754e-06,
      "loss": 1.6941,
      "step": 1250
    },
    {
      "epoch": 2.6195426195426195,
      "grad_norm": 6.124576568603516,
      "learning_rate": 2.5363825363825367e-06,
      "loss": 1.6758,
      "step": 1260
    },
    {
      "epoch": 2.64033264033264,
      "grad_norm": 4.872644424438477,
      "learning_rate": 2.397782397782398e-06,
      "loss": 1.6978,
      "step": 1270
    },
    {
      "epoch": 2.6611226611226613,
      "grad_norm": 4.484386444091797,
      "learning_rate": 2.2591822591822595e-06,
      "loss": 1.6581,
      "step": 1280
    },
    {
      "epoch": 2.681912681912682,
      "grad_norm": 7.658189296722412,
      "learning_rate": 2.120582120582121e-06,
      "loss": 1.7183,
      "step": 1290
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 5.719260215759277,
      "learning_rate": 1.9819819819819822e-06,
      "loss": 1.7106,
      "step": 1300
    },
    {
      "epoch": 2.7234927234927238,
      "grad_norm": 4.1074748039245605,
      "learning_rate": 1.8433818433818434e-06,
      "loss": 1.7132,
      "step": 1310
    },
    {
      "epoch": 2.7442827442827444,
      "grad_norm": 6.010272979736328,
      "learning_rate": 1.704781704781705e-06,
      "loss": 1.6926,
      "step": 1320
    },
    {
      "epoch": 2.765072765072765,
      "grad_norm": 4.118675231933594,
      "learning_rate": 1.5661815661815664e-06,
      "loss": 1.6429,
      "step": 1330
    },
    {
      "epoch": 2.785862785862786,
      "grad_norm": 5.510700225830078,
      "learning_rate": 1.4275814275814275e-06,
      "loss": 1.6842,
      "step": 1340
    },
    {
      "epoch": 2.8066528066528065,
      "grad_norm": 10.544052124023438,
      "learning_rate": 1.288981288981289e-06,
      "loss": 1.6551,
      "step": 1350
    },
    {
      "epoch": 2.8274428274428276,
      "grad_norm": 4.925736904144287,
      "learning_rate": 1.1503811503811505e-06,
      "loss": 1.6438,
      "step": 1360
    },
    {
      "epoch": 2.8482328482328483,
      "grad_norm": 4.999295711517334,
      "learning_rate": 1.0117810117810119e-06,
      "loss": 1.783,
      "step": 1370
    },
    {
      "epoch": 2.869022869022869,
      "grad_norm": 5.624696731567383,
      "learning_rate": 8.731808731808733e-07,
      "loss": 1.651,
      "step": 1380
    },
    {
      "epoch": 2.88981288981289,
      "grad_norm": 4.859792709350586,
      "learning_rate": 7.345807345807346e-07,
      "loss": 1.6228,
      "step": 1390
    },
    {
      "epoch": 2.9106029106029108,
      "grad_norm": 7.111476421356201,
      "learning_rate": 5.95980595980596e-07,
      "loss": 1.7713,
      "step": 1400
    },
    {
      "epoch": 2.9313929313929314,
      "grad_norm": 5.508101940155029,
      "learning_rate": 4.5738045738045745e-07,
      "loss": 1.6904,
      "step": 1410
    },
    {
      "epoch": 2.952182952182952,
      "grad_norm": 4.276683330535889,
      "learning_rate": 3.187803187803188e-07,
      "loss": 1.6903,
      "step": 1420
    },
    {
      "epoch": 2.972972972972973,
      "grad_norm": 4.696247577667236,
      "learning_rate": 1.801801801801802e-07,
      "loss": 1.6823,
      "step": 1430
    },
    {
      "epoch": 2.993762993762994,
      "grad_norm": 5.955237865447998,
      "learning_rate": 4.158004158004159e-08,
      "loss": 1.6586,
      "step": 1440
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.27233844000311064,
      "eval_f1": 0.27233844000311064,
      "eval_loss": 1.734084129333496,
      "eval_precision": 0.27233844000311064,
      "eval_recall": 0.27233844000311064,
      "eval_runtime": 19.7382,
      "eval_samples_per_second": 129.799,
      "eval_steps_per_second": 8.157,
      "step": 1443
    }
  ],
  "logging_steps": 10,
  "max_steps": 1443,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6024577977308160.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
